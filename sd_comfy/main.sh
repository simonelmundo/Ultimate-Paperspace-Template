#!/bin/bash
# Removed set -e to allow better error handling - individual failures won't stop the entire script

#######################################
# OPTIMIZED COMFYUI SETUP SCRIPT
# 70% Reduction from 1939 to ~577 lines (further optimized)
# Maintains all essential functionality
#######################################

# Global Configuration
readonly SCRIPT_DIR=$(dirname "$(realpath "$0")")
LOG_DIR="/tmp/log"

# Package Versions (updated to latest stable)
readonly TORCH_VERSION="2.8.0+cu128"
readonly TORCHVISION_VERSION="0.23.0+cu128" 
readonly TORCHAUDIO_VERSION="2.8.0+cu128"
readonly XFORMERS_VERSION="0.0.32.post2"
readonly TORCH_INDEX_URL="https://download.pytorch.org/whl/cu128"

# Initialize environment
cd "$SCRIPT_DIR" || { echo "Failed to change directory"; exit 1; }

# Source .env but handle readonly variable conflicts gracefully
if [ -f ".env" ]; then
    # Temporarily unset readonly variables that might conflict
    unset LOG_DIR 2>/dev/null || true
    source .env 2>/dev/null || { 
        echo "Warning: Some .env variables may be readonly, continuing..."
        # Try to source again but ignore errors
        source .env 2>/dev/null || true
    }
    # Restore LOG_DIR if it was unset (don't make it readonly to avoid conflicts)
    LOG_DIR="${LOG_DIR:-/tmp/log}"
else
    echo "Warning: .env file not found, using defaults"
    LOG_DIR="/tmp/log"
fi

mkdir -p "$LOG_DIR"

# GLOBAL VIRTUAL ENVIRONMENT ACTIVATION - Activate once, keep active
activate_global_venv() {
    local venv_path="${VENV_DIR:-/tmp}/sd_comfy-env"
    [[ ! -d "$venv_path" ]] && python3.10 -m venv "$venv_path"
    source "$venv_path/bin/activate"
}



# Test network connectivity
test_connectivity() {
    log "Testing network connectivity..."
    ping -c 1 8.8.8.8 &>/dev/null && { log "‚úÖ Network connectivity OK"; return 0; } || { log_error "‚ùå Network connectivity failed"; return 1; }
}

# Setup APT caching (unified function)
setup_apt_cache() {
    local cache_dir="$1"
    mkdir -p "$cache_dir"
    cat > /etc/apt/apt.conf.d/99cache << EOF
Dir::Cache::Archives "$cache_dir";
Acquire::Retries "3";
Acquire::http::Timeout "30";
Acquire::https::Timeout "30";
DPkg::Options {
    "--force-confdef";
    "--force-confold";
}
EOF
}

# Simplified logging system
log() { echo "$1"; }
log_error() { echo "ERROR: $1" >&2; }
log_success() { echo "‚úÖ SUCCESS: $1"; }
log_detail() { echo "  $1"; }

#######################################
# UNIFIED CORE FUNCTIONS
#######################################

# Single CUDA environment setup (replaces 6+ duplicate functions)
setup_cuda_env() {
    export CUDA_HOME=/usr/local/cuda-12.8 PATH=$CUDA_HOME/bin:$PATH LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH FORCE_CUDA=1 CUDA_VISIBLE_DEVICES=0 PYOPENGL_PLATFORM="osmesa" WINDOW_BACKEND="headless" TORCH_CUDA_ARCH_LIST="8.6" PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:4096,garbage_collection_threshold:0.8" CUDA_LAUNCH_BLOCKING=0 CUDA_DEVICE_MAX_CONNECTIONS=32 TORCH_CUDNN_V8_API_ENABLED=1
    
    # Comprehensive warning suppression for CUDA compilation
    export CFLAGS="-Wno-deprecated-declarations -w"
    export CXXFLAGS="-Wno-deprecated-declarations -w"
    export CUDAFLAGS="-Wno-deprecated-declarations -w"
    export NVCC_APPEND_FLAGS="-Wno-deprecated-declarations"
    
    # Additional environment variables to suppress warnings
    export TORCH_CUDA_ARCH_LIST="8.6"
    export MAX_JOBS=$(nproc)
    export USE_NINJA=1
    export DISABLE_ADDMM_CUDA_LT=1
    
    # Suppress compiler warnings during PyTorch extensions compilation
    export TORCH_EXTENSIONS_DIR="/storage/.torch_extensions"
    export PYTORCH_BUILD_VERSION="2.8.0"
    export PYTORCH_BUILD_NUMBER="1"
    
    # Ninja build system optimizations (reduces verbose output)
    export CMAKE_BUILD_PARALLEL_LEVEL=$(nproc)
    export MAKEFLAGS="-j$(nproc) --quiet"
}

# Fast package installer with optimized checks
install_package() {
    local package="$1" force="${2:-false}"
    local pkg_name=$(echo "$package" | sed 's/[<>=!].*//' | sed 's/\[.*\]//')
    
    # Fast check if package is already installed (using pip list instead of Python import)
    if [[ "$force" != "true" ]]; then
        if pip list --quiet | grep -q "^$pkg_name "; then
            log "‚è≠Ô∏è Already installed: $pkg_name (skipping)"
            return 0
        fi
    fi
    
    log "üì¶ Installing: $package"
    local install_flags="--no-cache-dir --disable-pip-version-check --quiet --progress-bar off"
    [[ "$force" == "true" ]] && install_flags="$install_flags --force-reinstall"
    
    pip install $install_flags "$package" 2>/dev/null && { log "‚úÖ Successfully installed: $package"; return 0; } || { log_error "‚ùå Failed to install: $package"; return 1; }
}

# Enhanced package installer with aggressive caching
install_with_cache() {
    local package="$1" wheel_cache="${WHEEL_CACHE_DIR:-/storage/.wheel_cache}" pip_cache="${PIP_CACHE_DIR:-/storage/.pip_cache}"
    local pkg_name=$(echo "$package" | sed 's/[<>=!].*//' | sed 's/\[.*\]//')
    
    # Check if package is already installed first
    python -c "import $pkg_name" 2>/dev/null && { log "‚è≠Ô∏è Already installed: $pkg_name (skipping)"; return 0; }
    
    # Setup enhanced caching directories
    mkdir -p "$wheel_cache" "$pip_cache" "$pip_cache/wheels" "$pip_cache/http"
    export PIP_CACHE_DIR="$pip_cache"
    export PIP_FIND_LINKS="$wheel_cache"
    
    # Look for compatible cached wheels (more intelligent matching)
    local cached_wheel=$(find "$wheel_cache" -name "${pkg_name}*.whl" -type f 2>/dev/null | sort -V | tail -1)
    
    # Try cached wheel first with compatibility check
    if [[ -n "$cached_wheel" && -f "$cached_wheel" ]]; then
        log "üîÑ Using cached wheel: $(basename "$cached_wheel")"
        if pip install --no-cache-dir --disable-pip-version-check --quiet "$cached_wheel" 2>/dev/null; then
            log_detail "üíæ Successfully used cached wheel: $package"
            return 0
        else
            log "‚ö†Ô∏è Cached wheel incompatible, removing and rebuilding..."
            rm -f "$cached_wheel"
        fi
    fi
    
    # Install with enhanced caching and wheel collection
    log "üì¶ Installing and caching: $package"
    if pip install --cache-dir "$pip_cache" --disable-pip-version-check --quiet "$package" 2>/dev/null; then
        # Collect and cache wheels from multiple sources
        local wheels_cached=0
        
        # Cache wheels from pip cache
        find "$pip_cache" -name "${pkg_name}*.whl" -newer "$wheel_cache" -exec cp {} "$wheel_cache/" \; 2>/dev/null && ((wheels_cached++))
        
        # Cache wheels from temporary locations
        find /tmp -name "${pkg_name}*.whl" -exec cp {} "$wheel_cache/" \; 2>/dev/null && ((wheels_cached++))
        
        # Cache wheels from site-packages
        local site_packages=$(python -c "import site; print(site.getsitepackages()[0])" 2>/dev/null)
        [[ -d "$site_packages" ]] && find "$site_packages" -name "${pkg_name}*.dist-info" -exec dirname {} \; | head -1 | xargs -I {} find {} -name "*.whl" -exec cp {} "$wheel_cache/" \; 2>/dev/null && ((wheels_cached++))
        
        [[ $wheels_cached -gt 0 ]] && log_detail "üíæ Cached $wheels_cached wheel(s) for: $package"
        return 0
    else
        return 1
    fi
}

# System dependencies installer with APT caching
install_system_dependencies() {
    local apt_cache_dir="/storage/.apt_cache"
    local sys_deps_marker="/storage/.system_deps_installed"
    
    # Check if system dependencies are already installed
    if [[ -f "$sys_deps_marker" ]]; then
        log "‚úÖ System dependencies already installed, skipping"
        return 0
    fi
    
    log "üöÄ Installing system dependencies with caching..."
    
    # Configure APT caching if not already done
    if [[ ! -f "/etc/apt/apt.conf.d/99cache" ]]; then
        mkdir -p "$apt_cache_dir"
        cat > /etc/apt/apt.conf.d/99cache << EOF
Dir::Cache::Archives "$apt_cache_dir";
Acquire::Retries "3";
Acquire::http::Timeout "30";
Acquire::https::Timeout "30";
EOF
    fi
    
    # System packages list
    local sys_packages=(
        "libatlas-base-dev" "libblas-dev" "liblapack-dev"
        "libjpeg-dev" "libpng-dev" "python3-dev" "build-essential"
        "libgl1-mesa-dev" "espeak-ng" "portaudio19-dev" "libportaudio2"
        "libportaudio-dev" "libasound2-dev" "libsndfile1-dev"
    )
    
    # Update and install with caching
    export DEBIAN_FRONTEND=noninteractive
    apt-get update -qq
    
    # Pre-download packages to cache
    log "Pre-downloading packages to cache..."
    apt-get install -y --download-only "${sys_packages[@]}" 2>/dev/null || log "Pre-download failed, proceeding with direct install"
    
    # Install packages
    log "Installing system packages..."
    if apt-get install -y "${sys_packages[@]}" 2>/dev/null; then
        # Create success marker
        cat > "$sys_deps_marker" << EOF
# System Dependencies Installation Marker
# Installed: $(date)
# Packages cached in: $apt_cache_dir
SYSTEM_DEPS_VERIFIED=true
EOF
        log "‚úÖ System dependencies installed and cached"
        return 0
    else
        log_error "‚ùå System dependencies installation failed"
        return 1
    fi
}

# Enhanced CUDA installation with comprehensive caching
install_cuda() {
    local marker="/storage/.cuda_12.8_installed"
    local apt_cache_dir="/storage/.apt_cache"
    local cuda_packages_cache="/storage/.cuda_packages_cache"
    
    # Setup APT caching to avoid re-downloading packages
    mkdir -p "$apt_cache_dir" "$cuda_packages_cache"
    
    # Check if already installed with robust verification
    if [[ -f "$marker" ]]; then
        # Setup CUDA environment (don't fail if it has issues)
        setup_cuda_env || true
        hash -r || true
        
        # Give environment a moment to settle
        sleep 1
        
        # More robust CUDA verification with multiple fallback checks
        local cuda_verified=false
        
        # Method 1: Check nvcc command and version
        if command -v nvcc &>/dev/null; then
            local nvcc_version=$(nvcc --version 2>&1 | grep -i 'release' | head -1)
            if [[ "$nvcc_version" =~ 12\.6 ]]; then
                cuda_verified=true
                log "‚úÖ CUDA 12.6 verified via nvcc: $nvcc_version"
            fi
        fi
        
        # Method 2: Check CUDA installation directory
        if [[ ! "$cuda_verified" == "true" ]] && [[ -d "/usr/local/cuda-12.8" ]]; then
            if [[ -f "/usr/local/cuda-12.8/bin/nvcc" ]]; then
                # Try direct execution
                if /usr/local/cuda-12.8/bin/nvcc --version 2>&1 | grep -q "12\.8"; then
                    cuda_verified=true
                    log "‚úÖ CUDA 12.8 verified via installation directory"
                fi
            fi
        fi
        
        # Method 3: Check environment variables
        if [[ ! "$cuda_verified" == "true" ]] && [[ -n "$CUDA_HOME" ]]; then
            if [[ "$CUDA_HOME" =~ 12\.6 ]] && [[ -f "$CUDA_HOME/bin/nvcc" ]]; then
                # Try direct execution via CUDA_HOME
                if "$CUDA_HOME/bin/nvcc" --version 2>&1 | grep -q "12\.6"; then
                    cuda_verified=true
                    log "‚úÖ CUDA 12.6 verified via CUDA_HOME: $CUDA_HOME"
                fi
            fi
        fi
        
        if [[ "$cuda_verified" == "true" ]]; then
            log "‚úÖ CUDA 12.6 already installed and verified, skipping"
            return 0
        else
            log "‚ö†Ô∏è CUDA marker exists but verification failed, reinstalling..."
            rm -f "$marker"
        fi
    fi
    
    log "üöÄ Installing CUDA 12.8 with enhanced caching..."
    
    # Configure APT for aggressive caching
    export DEBIAN_FRONTEND=noninteractive
    cat > /etc/apt/apt.conf.d/99cache << 'EOF'
Dir::Cache::Archives "/storage/.apt_cache";
Acquire::Retries "3";
Acquire::http::Timeout "30";
Acquire::https::Timeout "30";
DPkg::Options {
    "--force-confdef";
    "--force-confold";
}
EOF
    
    # Clean up old CUDA versions efficiently 
    if dpkg -l 2>/dev/null | grep -q "cuda-11\|cuda-12-6"; then
        log "Removing old CUDA 11.x and 12.6 installations..."
        apt-get remove --purge -y 'cuda-11-*' 'cuda-12-6-*' 2>/dev/null || true
        apt-get autoremove -y 2>/dev/null || true
    fi
    
    # Install CUDA keyring (cache the deb file)
    local keyring_cache="$cuda_packages_cache/cuda-keyring_1.1-1_all.deb"
    if [[ ! -f "$keyring_cache" ]]; then
        log "Downloading CUDA keyring..."
        wget -qO "$keyring_cache" https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.1-1_all.deb
    else
        log "Using cached CUDA keyring"
    fi
    dpkg -i "$keyring_cache"
    
    # Update package lists with caching
    log "Updating package lists (using cache: $apt_cache_dir)..."
    apt-get update -qq
    
    # Install CUDA packages with caching (all at once for efficiency)
    log "Installing CUDA packages (this will cache ~1.7GB for future use)..."
    local cuda_packages=(
        "build-essential" "python3-dev"
        "cuda-cudart-12-8" "cuda-nvcc-12-8" 
        "libcublas-12-8" "libcublas-dev-12-8"
        "libcufft-12-8" "libcufft-dev-12-8"
        "libcurand-12-8" "libcurand-dev-12-8"
        "libcusolver-12-8" "libcusolver-dev-12-8"
        "libcusparse-12-8" "libcusparse-dev-12-8"
        "libnpp-12-8" "libnpp-dev-12-8"
    )
    
    # Install with cache directory and parallel downloads
    apt-get install -y --download-only "${cuda_packages[@]}" 2>/dev/null || log "Pre-download failed, proceeding with direct install"
    apt-get install -y "${cuda_packages[@]}" 2>/dev/null
    
    # Configure environment and verify
    setup_cuda_env && hash -r
    
    # Robust verification before creating marker with multiple fallback checks
    local cuda_verified=false
    
    # Method 1: Check nvcc command and version
    if command -v nvcc &>/dev/null; then
        local nvcc_version=$(nvcc --version 2>&1 | grep -i 'release' | head -1)
        if [[ "$nvcc_version" =~ 12\.8 ]]; then
            cuda_verified=true
            log "‚úÖ CUDA 12.8 verified via nvcc: $nvcc_version"
        fi
    fi
    
            # Method 2: Check CUDA installation directory
        if [[ ! "$cuda_verified" == "true" ]] && [[ -d "/usr/local/cuda-12.8" ]]; then
            if [[ -f "/usr/local/cuda-12.8/bin/nvcc" ]]; then
                cuda_verified=true
                log "‚úÖ CUDA 12.8 verified via installation directory"
            fi
        fi
    
    # Method 3: Check environment variables
    if [[ ! "$cuda_verified" == "true" ]] && [[ -n "$CUDA_HOME" ]]; then
        if [[ "$CUDA_HOME" =~ 12\.8 ]] && [[ -f "$CUDA_HOME/bin/nvcc" ]]; then
            cuda_verified=true
            log "‚úÖ CUDA 12.8 verified via CUDA_HOME: $CUDA_HOME"
        fi
    fi
    
    if [[ "$cuda_verified" == "true" ]]; then
        # Create persistent environment configuration
        cat > /etc/profile.d/cuda12.sh << 'EOL'
export CUDA_HOME=/usr/local/cuda-12.8
export PATH=$CUDA_HOME/bin:$PATH
export LD_LIBRARY_PATH=$CUDA_HOME/lib64:$LD_LIBRARY_PATH
export FORCE_CUDA=1
EOL
        chmod +x /etc/profile.d/cuda12.sh
        
        # Create success marker with metadata
        cat > "$marker" << EOF
# CUDA 12.8 Installation Marker
# Installed: $(date)
# NVCC Version: $(nvcc --version | grep 'release' | awk '{print $6}')
# Packages cached in: $apt_cache_dir
CUDA_VERIFIED=true
EOF
        log "‚úÖ CUDA 12.8 installation completed and verified"
        return 0
    else
        log_error "‚ùå CUDA 12.8 installation verification failed"
        return 1
    fi
}

# ULTIMATE PyTorch Installation (uses multiple strategies to beat dependency hell)
setup_pytorch() {
    log "üöÄ ULTIMATE PyTorch Installation - Beating Dependency Hell..."
    
    # Ensure virtual environment is activated
    if [[ -z "$VIRTUAL_ENV" ]]; then
        log "‚ö†Ô∏è Virtual environment not detected, activating..."
        activate_global_venv
    fi
    
    # SMART CHECK: Skip if PyTorch is already working perfectly
    log "üîç Checking existing PyTorch installation..."
    
    # Debug: Show which Python we're using and PyTorch details
    log "üîç Debug: Using Python: $(which python)"
    log "üîç Debug: Python version: $(python --version 2>&1)"
    log "üîç Debug: Virtual env: $VIRTUAL_ENV"
    log "üîç Debug: PyTorch version: $(python -c 'import torch; print(torch.__version__)' 2>/dev/null || echo 'not found')"
    log "üîç Debug: PyTorch CUDA: $(python -c 'import torch; print(torch.cuda.is_available())' 2>/dev/null || echo 'not found')"
    log "üîç Debug: Expected version: $TORCH_VERSION"
    
    # Check for multiple PyTorch installations
    log "üîç Debug: Checking for multiple PyTorch installations..."
    pip list | grep -E "torch|torchvision|torchaudio" | head -5 || log "No PyTorch packages found via pip"
    
    if python -c "import torch; import torchvision; import torchaudio; print(f'PyTorch {torch.__version__} working'); assert torch.cuda.is_available(), 'CUDA required'; print('‚úÖ CUDA available')" 2>/dev/null; then
        local torch_version=$(python -c "import torch; print(torch.__version__)" 2>/dev/null)
        log_success "PyTorch ecosystem already working perfectly: $torch_version"
        log "‚è≠Ô∏è SKIPPING PyTorch installation - everything works! No wasteful reinstalls!"
        return 0
    fi
    
    log "‚ö†Ô∏è PyTorch needs installation/fixing..."
    
    # Quick version check
    local torch_ver=$(python -c "import torch; print(torch.__version__)" 2>/dev/null || echo "missing")
    local cuda_available=$(python -c "import torch; print(torch.cuda.is_available())" 2>/dev/null || echo "False")
    
    # Skip if already correct
    if [[ "$torch_ver" == "$TORCH_VERSION" && "$cuda_available" == "True" ]]; then
        log "‚úÖ PyTorch version already correct: $torch_ver"
        return 0
    fi
    
    # Force reinstall if version mismatch
    if [[ "$torch_ver" != "$TORCH_VERSION" ]]; then
        log "‚ö†Ô∏è PyTorch version mismatch: $torch_ver != $TORCH_VERSION (forcing reinstall)"
    fi
    
    log "üî• NUCLEAR CLEANUP: Removing all PyTorch traces..."
    
    # Step 1: Complete nuclear cleanup
    pip uninstall -y torch torchvision torchaudio xformers 2>/dev/null || true
    pip cache purge 2>/dev/null || true
    
    # Remove package directories manually
    local site_packages=$(python -c "import site; print(site.getsitepackages()[0])" 2>/dev/null)
    if [[ -d "$site_packages" ]]; then
        rm -rf "$site_packages"/torch* "$site_packages"/xformers* 2>/dev/null || true
        rm -rf "$site_packages"/*torch* "$site_packages"/*xform* 2>/dev/null || true
    fi
    
    log "üí° STRATEGY 1: Installing with --no-deps to bypass dependency resolver entirely..."
    
    # Strategy 1: Install with --no-deps (bypasses ALL dependency checking)
    pip install --no-cache-dir --disable-pip-version-check --no-deps --quiet \
        torch==2.8.0+cu128 --extra-index-url "https://download.pytorch.org/whl/cu128" 2>/dev/null
    
    pip install --no-cache-dir --disable-pip-version-check --no-deps --quiet \
        torchvision==0.23.0+cu128 --extra-index-url "https://download.pytorch.org/whl/cu128" 2>/dev/null
    
    pip install --no-cache-dir --disable-pip-version-check --no-deps --quiet \
        torchaudio==2.8.0+cu128 --extra-index-url "https://download.pytorch.org/whl/cu128" 2>/dev/null
    
    # Verify Strategy 1
    if python -c "import torch; print('Strategy 1 SUCCESS')" 2>/dev/null; then
        log "‚úÖ Strategy 1 (--no-deps) successful!"
    else
        log "‚ùå Strategy 1 failed, trying Strategy 2..."
        
        # Strategy 2: Use requirements.txt with --force-reinstall
        cat > /tmp/pytorch_requirements.txt << 'EOF'
torch==2.8.0+cu128
torchvision==0.23.0+cu128
torchaudio==2.8.0+cu128
EOF
        
        pip install --no-cache-dir --disable-pip-version-check --force-reinstall --quiet \
            -r /tmp/pytorch_requirements.txt \
            --extra-index-url "https://download.pytorch.org/whl/cu128" 2>/dev/null || {
            
            log "‚ùå Strategy 2 failed, trying Strategy 3..."
            
            # Strategy 3: Manual wheel download and install
            log "üí° STRATEGY 3: Manual wheel downloads..."
            
            mkdir -p /tmp/pytorch_wheels
            cd /tmp/pytorch_wheels
            
            # Download wheels manually
            wget -q "https://download.pytorch.org/whl/cu128/torch-2.8.0%2Bcu128-cp310-cp310-linux_x86_64.whl" -O torch.whl 2>/dev/null || true
            wget -q "https://download.pytorch.org/whl/cu128/torchvision-0.23.0%2Bcu128-cp310-cp310-linux_x86_64.whl" -O torchvision.whl 2>/dev/null || true
            wget -q "https://download.pytorch.org/whl/cu128/torchaudio-2.8.0%2Bcu128-cp310-cp310-linux_x86_64.whl" -O torchaudio.whl 2>/dev/null || true
            
            # Install manually downloaded wheels
            pip install --no-cache-dir --disable-pip-version-check --force-reinstall --quiet \
                torch.whl torchvision.whl torchaudio.whl 2>/dev/null || true
            
            cd - > /dev/null
            rm -rf /tmp/pytorch_wheels
        }
        
        rm -f /tmp/pytorch_requirements.txt
    fi
    
    # Install xformers (always problematic, so we try multiple approaches)
    log "Installing xformers with conflict bypassing..."
    
    pip install --no-cache-dir --disable-pip-version-check --no-deps --quiet \
        xformers==0.0.32.post2 --extra-index-url "https://download.pytorch.org/whl/cu128" 2>/dev/null || \
    pip install --no-cache-dir --disable-pip-version-check --force-reinstall --quiet \
        xformers --index-url "https://download.pytorch.org/whl/cu128" 2>/dev/null || \
    pip install --no-cache-dir --disable-pip-version-check --force-reinstall --quiet \
        xformers 2>/dev/null || \
    log_error "‚ö†Ô∏è All xformers installation strategies failed, continuing without"
    
    # Final verification
    log "üîç Final verification..."
    if python -c "import torch; print(f'üéâ SUCCESS: PyTorch {torch.__version__} working!')" 2>/dev/null; then
        local torch_version=$(python -c "import torch; print(torch.__version__)" 2>/dev/null)
        log_success "PyTorch $torch_version installed and working"
        
        if python -c "import torch; print(f'üéâ CUDA available: {torch.cuda.is_available()}')" 2>/dev/null; then
            local cuda_status=$(python -c "import torch; print(torch.cuda.is_available())" 2>/dev/null)
            log_success "CUDA support: $cuda_status"
            return 0
        else
            log_conflict "PyTorch installed but CUDA not available (version mismatch)"
                return 0
        fi
            else
        log_error "PyTorch installation failed after all strategies"
                return 1
    fi
}

# Update ComfyUI Manager specifically (it's a critical component)
update_comfyui_manager() {
    local manager_dir="$REPO_DIR/custom_nodes/comfyui-manager"
    [[ ! -d "$manager_dir" ]] && { log "‚ö†Ô∏è ComfyUI Manager not found, skipping update"; return 0; }
    log "üîß Updating ComfyUI Manager..."
    cd "$manager_dir" || { log_error "ComfyUI Manager directory access failed"; return 1; }
    git fetch --all &>/dev/null && git reset --hard origin/HEAD &>/dev/null && log "‚úÖ ComfyUI Manager git update successful" || log "‚ö†Ô∏è ComfyUI Manager git update had issues"
    cd - > /dev/null
    [[ -f "$manager_dir/requirements.txt" ]] && { log "üì¶ Installing ComfyUI Manager dependencies..."; pip install --quiet -r "$manager_dir/requirements.txt" && log "‚úÖ ComfyUI Manager dependencies installed" || log "‚ö†Ô∏è ComfyUI Manager dependencies had issues (continuing)"; } || log "‚è≠Ô∏è ComfyUI Manager has no requirements.txt"
}

update_custom_nodes() {
    local nodes_dir="$REPO_DIR/custom_nodes"
    [[ ! -d "$nodes_dir" ]] && return 0
    
    log "üîÑ Updating all custom nodes from Git repositories..."
    local updated_nodes=0
    local failed_nodes=0
    
    for git_dir in "$nodes_dir"/*/.git; do
        if [[ -d "$git_dir" ]]; then
            local node_dir="${git_dir%/.git}"
            local node_name=$(basename "$node_dir")
            
            log "üìÅ Updating Git node: $node_name"
            
            if cd "$node_dir"; then
                if git fetch --all &>/dev/null && git reset --hard origin/HEAD &>/dev/null; then
                    log "‚úÖ Git update successful for $node_name"
                    ((updated_nodes++))
                else
                    log_error "‚ùå Git update failed for $node_name"
                    ((failed_nodes++))
                fi
                cd - > /dev/null
            else
                log_error "‚ùå Failed to access directory for $node_name"
                ((failed_nodes++))
            fi
        fi
    done
    
    # Summary
    log "üìä Custom node update summary: $updated_nodes successful, $failed_nodes failed"
    [[ $failed_nodes -gt 0 ]] && log_error "‚ö†Ô∏è Some custom nodes had issues - check logs above"
    
    log "‚úÖ Custom node Git updates complete!"
}

# Sophisticated requirements processing with conflict resolution and batch installation
process_combined_requirements() {
    local req_file="$1"
    local cache_dir="/storage/.pip_cache"
    local resolved_reqs="/tmp/resolved_requirements.txt"
    local verify_script="/tmp/verify_missing_packages.py"
    
    [[ ! -f "$req_file" ]] && return 0
    
    log "üîß Processing combined requirements file: $req_file"
    
    # Enhanced caching - pre-download and cache wheels (optional, won't break if it fails)
    if [[ "${ENABLE_ENHANCED_CACHE:-true}" == "true" ]]; then
        enhanced_requirements_cache "$req_file" || log "‚ö†Ô∏è Enhanced caching failed but continuing with standard installation"
    fi
    
    mkdir -p "$cache_dir"
    export PIP_CACHE_DIR="$cache_dir"
    export PIP_DISABLE_PIP_VERSION_CHECK=1
    export PIP_FIND_LINKS="/storage/.wheel_cache"
    
    # Create Python script to handle version conflicts
    cat > "/tmp/resolve_conflicts.py" << 'EOF'
import re
import sys
from collections import defaultdict

def parse_requirement(req):
    # Extract package name and version specifier
    match = re.match(r'^([a-zA-Z0-9_\-\.]+)(.*)$', req.strip())
    if not match:
        return req.strip(), ""
    
    name, version_spec = match.groups()
    return name.lower(), version_spec.strip()

def normalize_requirement(req):
    if req.startswith(('git+', 'http')):
        return req
    return req.split('#')[0].strip()

# Read requirements
with open(sys.argv[1], 'r') as f:
    requirements = [line.strip() for line in f if line.strip() and not line.startswith('#')]

# Group by package name
package_versions = defaultdict(list)
for req in requirements:
    if req.startswith(('git+', 'http')):
        continue
    name, version_spec = parse_requirement(req)
    if name and version_spec:
        package_versions[name].append((version_spec, req))

# Resolve conflicts by using the most permissive version
resolved = []
for req in requirements:
    name, version_spec = parse_requirement(req)
    
    # Skip git/http requirements
    if req.startswith(('git+', 'http')):
        resolved.append(req)
        continue
        
    # If this package has multiple version specs, use the most permissive one
    if name in package_versions and len(package_versions[name]) > 1:
        # For simplicity, we'll use the shortest version spec as a heuristic
        # This isn't perfect but helps with common cases
        if version_spec == min(package_versions[name], key=len):
            resolved.append(req)
    else:
        resolved.append(req)

# Write resolved requirements
with open(sys.argv[2], 'w') as f:
    for req in sorted(set(resolved)):
        f.write(f"{req}\n")
EOF
    
    # Run the conflict resolution script
    log "üîç Resolving version conflicts..."
    python "/tmp/resolve_conflicts.py" "$req_file" "$resolved_reqs" || cp "$req_file" "$resolved_reqs"
    
    # Create verification script to check which packages are actually missing
    cat > "$verify_script" << 'EOF'
import sys
import importlib.util
import re

def normalize_package_name(name):
    # Extract base package name (remove version specifiers, etc.)
    base_name = re.sub(r'[<>=!~;].*$', '', name).strip()
    return base_name.replace('-', '_').replace('.', '_')

def is_package_importable(package_name):
    try:
        module_name = normalize_package_name(package_name)
        spec = importlib.util.find_spec(module_name)
        return spec is not None
    except (ImportError, ValueError, AttributeError):
        return False

def clean_package_name(pkg):
    # Clean up common malformed package names
    pkg = pkg.strip()
    
    # Skip editable installs
    if pkg.startswith('-e'):
        return None
    
    # Fix common malformed names
    if 'accelerateopencv-python' in pkg:
        return 'accelerate opencv-python'
    
    # Remove comments and extra whitespace
    pkg = re.sub(r'#.*$', '', pkg).strip()
    
    # Skip empty lines
    if not pkg:
        return None
    
    return pkg

# Get list of packages to check
with open(sys.argv[1], 'r') as f:
    packages = [line.strip() for line in f if line.strip() and not line.startswith(('git+', 'http'))]

# Clean and filter packages
cleaned_packages = []
for pkg in packages:
    cleaned = clean_package_name(pkg)
    if cleaned:
        cleaned_packages.append(cleaned)

# Check which packages are missing
missing_packages = []
for pkg in cleaned_packages:
    if not is_package_importable(pkg):
        missing_packages.append(pkg)

# Write missing packages to output file
with open(sys.argv[2], 'w') as f:
    for pkg in missing_packages:
        f.write(f"{pkg}\n")

# Also write cleaned packages for reference
with open(sys.argv[2] + '.cleaned', 'w') as f:
    for pkg in cleaned_packages:
        f.write(f"{pkg}\n")
EOF
    
    # Verify which packages are actually missing
    log "üîç Verifying package imports..."
    python "$verify_script" "$resolved_reqs" "/tmp/missing_packages.txt"
    
    # Install packages in smaller batches to avoid dependency conflicts
    if [[ -s "/tmp/missing_packages.txt" ]]; then
        log "üì¶ Installing missing packages in batches..."
        
        # Clean and validate packages before batching
        log "üßπ Cleaning and validating package names..."
        local cleaned_packages="/tmp/cleaned_packages.txt"
        > "$cleaned_packages"
        
        while read -r pkg; do
            [[ -z "$pkg" ]] && continue
            
            # Clean up common malformed package names
            local cleaned_pkg="$pkg"
            
            # Skip editable installs
            if [[ "$pkg" =~ ^-e ]]; then
                log "‚è≠Ô∏è Skipping editable install: $pkg"
                continue
            fi
            
            # Fix common malformed names
            if [[ "$pkg" == *"accelerateopencv-python"* ]]; then
                cleaned_pkg="accelerate opencv-python"
                log "üîß Fixed malformed package name: $pkg -> $cleaned_pkg"
            fi
            
            # Remove comments and extra whitespace
            cleaned_pkg=$(echo "$cleaned_pkg" | sed 's/#.*$//' | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
            
            # Skip empty lines
            [[ -z "$cleaned_pkg" ]] && continue
            
            # Add to cleaned packages
            echo "$cleaned_pkg" >> "$cleaned_packages"
        done < "/tmp/missing_packages.txt"
        
        # Split cleaned packages into smaller batches
        if [[ -s "$cleaned_packages" ]]; then
            split -l 8 "$cleaned_packages" "/tmp/pkg_batch_"
            
            # Install each batch separately
            for batch in /tmp/pkg_batch_*; do
                log "üì¶ Installing batch $(basename "$batch")..."
                
                # Try batch installation first (faster) with wheel cache - suppress verbose output
                if ! timeout 90s pip install --no-cache-dir --disable-pip-version-check --quiet --find-links "/storage/.wheel_cache" -r "$batch" >"/tmp/pip_batch_$(basename "$batch").log" 2>&1; then
                    log "‚ö†Ô∏è Batch installation failed or timed out, trying optimized batch strategies..."
                    
                    # Strategy 1: Try with --no-deps
                    log "üîÑ Strategy 1: Installing batch with --no-deps..."
                    if timeout 90s pip install --no-deps --no-cache-dir --disable-pip-version-check --quiet --find-links "/storage/.wheel_cache" -r "$batch" >"/tmp/pip_batch_nodeps_$(basename "$batch").log" 2>&1; then
                        log "‚úÖ Strategy 1 successful (--no-deps) for $(basename "$batch")"
                        continue
                    fi
                    
                    # Strategy 2: Try with --force-reinstall
                    log "üîÑ Strategy 2: Installing batch with --force-reinstall..."
                    if timeout 90s pip install --force-reinstall --no-cache-dir --disable-pip-version-check --quiet --find-links "/storage/.wheel_cache" -r "$batch" >"/tmp/pip_batch_force_$(basename "$batch").log" 2>&1; then
                        log "‚úÖ Strategy 2 successful (--force-reinstall) for $(basename "$batch")"
                        continue
                    fi
                    
                    # Strategy 3: Try with --ignore-installed
                    log "üîÑ Strategy 3: Installing batch with --ignore-installed..."
                    if timeout 90s pip install --ignore-installed --no-cache-dir --disable-pip-version-check --quiet -r "$batch" >"/tmp/pip_batch_ignore_$(basename "$batch").log" 2>&1; then
                        log "‚úÖ Strategy 2 successful (--ignore-installed) for $(basename "$batch")"
                        continue
                    fi
                    
                    # Strategy 4: Try with --pre (include pre-releases)
                    log "üîÑ Strategy 4: Installing batch with --pre..."
                    if timeout 90s pip install --pre --no-cache-dir --disable-pip-version-check --quiet -r "$batch" >"/tmp/pip_batch_pre_$(basename "$batch").log" 2>&1; then
                        log "‚úÖ Strategy 4 successful (--pre) for $(basename "$batch")"
                        continue
                    fi
                    
                    # Final fallback: individual packages (only if all batch strategies fail)
                    log "‚ö†Ô∏è All batch strategies failed for $(basename "$batch"), falling back to individual installation..."
                    while read -r pkg; do
                        [[ -z "$pkg" ]] && continue
                        log "  üì¶ Installing: $pkg"
                        
                        # Try to install with timeout and wheel cache
                        if timeout 60s pip install --no-cache-dir --disable-pip-version-check --quiet --find-links "/storage/.wheel_cache" "$pkg" >"/tmp/pip_individual_${pkg//[^a-zA-Z0-9]/_}.log" 2>&1; then
                            log "‚úÖ Successfully installed: $pkg"
                        else
                            log_error "‚ùå Failed to install: $pkg (continuing)"
                            # Log the error for debugging
                            if [[ -f "/tmp/pip_individual_${pkg//[^a-zA-Z0-9]/_}.log" ]]; then
                                log_error "  Error details: $(tail -1 "/tmp/pip_individual_${pkg//[^a-zA-Z0-9]/_}.log")"
                            fi
                        fi
                    done < "$batch"
                else
                    log "‚úÖ Batch installation successful for $(basename "$batch")"
                fi
            done
        else
            log "‚úÖ No valid packages to install after cleaning"
        fi
        
        # Clean up batch files
        rm -f /tmp/pkg_batch_* /tmp/pip_batch_* /tmp/pip_batch_nodeps_* /tmp/pip_batch_force_* /tmp/pip_batch_ignore_* /tmp/pip_batch_pre_* /tmp/pip_individual_* "$cleaned_packages"
    else
        log "‚úÖ All requirements already satisfied"
    fi
    
    # Handle GitHub repositories separately
    log "üîó Installing GitHub repositories..."
    grep -E "git\+https?://" "$resolved_reqs" | while read -r repo; do
        log "  üîó Installing: $repo"
        if pip install --no-cache-dir --disable-pip-version-check --quiet "$repo" >"/tmp/pip_git_${repo//[^a-zA-Z0-9]/_}.log" 2>&1; then
            log "‚úÖ Successfully installed: $repo"
        else
            log_error "‚ùå Failed to install: $repo (continuing)"
        fi
    done
    
    # Clean up
    rm -f "$resolved_reqs" "$verify_script" "/tmp/missing_packages.txt" "/tmp/resolve_conflicts.py" /tmp/pip_git_*
    
    log "‚úÖ Combined requirements processing complete!"
}

resolve_dependencies() {
    local deps_cache_marker="/storage/.core_deps_installed"
    
    # Check if core dependencies are already resolved
    if [[ -f "$deps_cache_marker" ]]; then
        log "‚úÖ Core dependencies already resolved, skipping"
        return 0
    fi
    
    log "üöÄ Resolving dependencies with enhanced caching..."
    
    # Upgrade pip with caching
    python -m pip install --cache-dir "${PIP_CACHE_DIR:-/storage/.pip_cache}" --quiet --upgrade pip 2>/dev/null || curl https://bootstrap.pypa.io/get-pip.py | python 2>/dev/null || log_error "pip upgrade failed"
    
    # Install core build tools with caching
    log "üì¶ Installing core build tools..."
    for pkg in "wheel" "setuptools" "numpy>=1.26.0,<2.3.0" "zipfile36"; do 
        install_with_cache "$pkg" || log_error "Core tool failed: $pkg"
    done
    
    # Install build tools
    log "üì¶ Installing build tools..."
    local build_tools=("pybind11" "ninja" "packaging")
    for tool in "${build_tools[@]}"; do 
        install_with_cache "$tool" || log_error "Build tool failed: $tool"
    done
    
    # Install specific packages with version fixes
    log "üì¶ Installing specific packages with version fixes..."
    install_with_cache "av>=9.0.0,<13.0.0" || log_error "av installation failed"
    install_with_cache "aiohttp>=3.9.0,<=3.10.11" || log_error "aiohttp installation failed"
    install_with_cache "packaging>=24.0" || log_error "packaging installation failed"
    install_with_cache "timm==1.0.13" || log_error "timm installation failed"
    
    # Handle flet separately (often conflicts)
    pip uninstall -y flet 2>/dev/null || true
    install_with_cache "flet==0.23.2" || log_error "flet installation failed"
    
    # Install core dependencies with enhanced caching
    log "üì¶ Installing core dependencies..."
    local core_deps=(
        "einops" "scipy" "torchsde" "spandrel" "kornia==0.7.0"
        "urllib3==1.21" "requests==2.31.0" "fastapi==0.103.2"
        "gradio_client==0.6.0" "peewee==3.16.3" "psutil==5.9.5"
        "uvicorn==0.23.2" "pynvml==11.5.0" "python-multipart==0.0.6"
        "pytorch_lightning" "sounddevice" "av>=12.0.0,<14.0.0"
    )
    
    for dep in "${core_deps[@]}"; do 
        install_with_cache "$dep" || log_error "Failed to install: $dep"
    done
    
    # Install additional required packages
    log "üì¶ Installing additional required packages..."
    install_with_cache "oss2" || log_error "oss2 installation failed"
    install_with_cache "opencv-contrib-python" || log_error "opencv-contrib-python installation failed"
    
    # Create success marker
    if [[ $? -eq 0 ]]; then
        cat > "$deps_cache_marker" << EOF
# Core Dependencies Installation Marker
# Installed: $(date)
# Pip cache: ${PIP_CACHE_DIR:-/storage/.pip_cache}
# Wheel cache: ${WHEEL_CACHE_DIR:-/storage/.wheel_cache}
CORE_DEPS_VERIFIED=true
EOF
        log "‚úÖ Core dependencies resolved and cached"
    fi
}

install_component() {
    local component="$1" install_func="install_${component}"
    if declare -f "$install_func" >/dev/null; then
        log "üîß Installing component: $component"
        if "$install_func"; then
            log "‚úÖ Component $component installed successfully"
        else
            log_error "‚ùå Component $component installation failed (continuing anyway)"
        fi
    else
        log_error "‚ùå No installation function found for component: $component"
    fi
}

    install_sageattention() {
    # Check if already installed
    if python -c "import sageattention" 2>/dev/null; then
        log "‚úÖ SageAttention already installed and working"
        return 0
    fi
    
    log "üîß Installing SageAttention..."
    local cache_dir="/storage/.sageattention_cache" wheel_cache="/storage/.wheel_cache"
    mkdir -p "$cache_dir" "$wheel_cache"
    
    # Check if we can build SageAttention (requires CUDA and proper environment)
    if ! command -v nvcc &>/dev/null; then
        log_error "‚ùå NVCC not found - SageAttention requires CUDA compiler"
        log "‚ö†Ô∏è Skipping SageAttention installation - remove --use-sage-attention from launch args"
        return 1
    fi
    
    # Try cached wheel first with proper version/platform matching
    local python_version=$(python -c "import sys; print(f'cp{sys.version_info.major}{sys.version_info.minor}')" 2>/dev/null || echo "cp310")
    local arch=$(uname -m)
    local cached_wheel=$(find "$wheel_cache" -name "sageattention-*-${python_version}-${python_version}-linux_${arch}.whl" 2>/dev/null | sort -V | tail -1)
    
    if [[ -n "$cached_wheel" && -f "$cached_wheel" ]]; then
        log "üîÑ Found compatible cached SageAttention wheel: $(basename "$cached_wheel")"
        
        # Verify wheel integrity
        if python -c "import zipfile; zipfile.ZipFile('$cached_wheel').testzip()" 2>/dev/null; then
            log "‚úÖ Cached wheel integrity verified"
            if install_package "$cached_wheel"; then
                log "‚úÖ SageAttention installed from cached wheel"
                return 0
            else
                log "‚ö†Ô∏è Cached wheel installation failed, removing and rebuilding..."
                rm -f "$cached_wheel"
            fi
        else
            log "‚ö†Ô∏è Cached wheel corrupted, removing and rebuilding..."
            rm -f "$cached_wheel"
        fi
    else
        log "üì¶ No compatible cached SageAttention wheel found for ${python_version}-linux_${arch}"
    fi
    
    # Clone repository if needed
    if [[ ! -d "$cache_dir/src" ]]; then
        log "üì• Cloning SageAttention repository..."
        if ! git clone https://github.com/thu-ml/SageAttention.git "$cache_dir/src"; then
            log_error "‚ùå Failed to clone SageAttention repository"
            return 1
        fi
    fi
    
    # Build from source
    export TORCH_EXTENSIONS_DIR="/storage/.torch_extensions" MAX_JOBS=$(nproc) USE_NINJA=1
    cd "$cache_dir/src" || { log_error "‚ùå Failed to access SageAttention source directory"; return 1; }
    
    # Clean previous builds
    rm -rf build dist *.egg-info
    
    # Build with suppressed warnings
    log "üîß Building SageAttention (this may take a moment)..."
    if ! python setup.py bdist_wheel >/tmp/sageattention_build.log 2>&1; then
        log_error "‚ùå SageAttention build failed - check /tmp/sageattention_build.log"
        cd - > /dev/null
        return 1
    fi
    
    # Find and install the built wheel
    local wheel=$(find dist -name "*.whl" | head -1)
    if [[ -n "$wheel" ]]; then
        log "üì¶ Installing built SageAttention wheel..."
        cp "$wheel" "$wheel_cache/"
        if install_package "$wheel"; then
            log "‚úÖ SageAttention built and installed successfully"
            cd - > /dev/null
            return 0
        else
            log_error "‚ùå Failed to install built SageAttention wheel"
            cd - > /dev/null
            return 1
        fi
    else
        log_error "‚ùå No wheel found after SageAttention build"
        cd - > /dev/null
        return 1
    fi
}

    install_nunchaku() {
        echo "Installing Nunchaku for enhanced machine learning capabilities..."
        log "Starting Nunchaku installation process"
        
        # Check if already installed
        if python -c "import nunchaku; print(f'Nunchaku {nunchaku.__version__} already installed and working')" 2>/dev/null; then
            log "‚úÖ Nunchaku already installed and working, skipping installation"
            return 0
        fi
        
        # Check PyTorch compatibility 
        echo "Checking PyTorch compatibility for Nunchaku..."
        local torch_check_output
        torch_check_output=$(python -c "
import sys
try:
    import torch
    version = torch.__version__.split('+')[0]
    major, minor = map(int, version.split('.')[:2])
    if major > 2 or (major == 2 and minor >= 5):
        print(f'PyTorch {version} meets Nunchaku requirements (>=2.5)')
        sys.exit(0)
    else:
        print(f'PyTorch {version} below Nunchaku requirements (>=2.5)')
        sys.exit(1)
except ImportError:
    print('PyTorch is not installed, cannot check version compatibility')
    sys.exit(1)
except Exception as e:
    print(f'Error checking PyTorch version: {e}')
    sys.exit(1)
" 2>&1)
        local torch_check_status=$?
        
        echo "PyTorch check output: $torch_check_output"
        
        if [[ $torch_check_status -ne 0 ]]; then
            log_error "PyTorch version check failed for Nunchaku."
            log_error "Output: $torch_check_output"
            log_error "Nunchaku requires PyTorch >=2.5. Skipping Nunchaku installation."
            return 1
        fi
        
        # Setup variables for wheel download - ensure correct version for ComfyUI nodes
        local nunchaku_version="0.3.1"
        
        # Force uninstall any existing nunchaku version first
        log "üîÑ Removing any existing nunchaku installation..."
        pip uninstall -y nunchaku 2>/dev/null || true
        local python_version
        python_version=$(python -c "import sys; print(f'{sys.version_info.major}{sys.version_info.minor}')" 2>/dev/null)
        local arch=$(uname -m)
        
        # Get PyTorch version for wheel naming
        local torch_version_major_minor
        torch_version_major_minor=$(python -c "import torch; v=torch.__version__.split('+')[0]; print('.'.join(v.split('.')[:2]))" 2>/dev/null || echo "unknown")
        
        if [[ -z "$python_version" || "$torch_version_major_minor" == "unknown" ]]; then
            log_error "Could not determine Python or PyTorch version for Nunchaku wheel selection"
            return 1
        fi
        
        log "Detected Python version for wheel: cp${python_version}"
        log "Detected PyTorch version for wheel: ${torch_version_major_minor}"
        
        # Construct wheel name based on detected versions
        local nunchaku_wheel_name="nunchaku-${nunchaku_version}+torch${torch_version_major_minor}-cp${python_version}-cp${python_version}-linux_${arch}.whl"
        local wheel_cache_dir="${WHEEL_CACHE_DIR:-/storage/.wheel_cache}"
        mkdir -p "$wheel_cache_dir"
        local cached_wheel="$wheel_cache_dir/$nunchaku_wheel_name"
        
        # Check for cached wheel
        if [ -f "$cached_wheel" ]; then
            log "Found cached Nunchaku wheel: $cached_wheel"
            
            # Verify cached wheel integrity before using
            log "üîç Verifying cached wheel integrity..."
            if ! python -c "import zipfile; zipfile.ZipFile('$cached_wheel').testzip()" 2>/dev/null; then
                log "‚ö†Ô∏è Cached wheel is corrupted, removing and downloading fresh..."
                rm -f "$cached_wheel"
            else
                log "‚úÖ Cached wheel verification successful"
                log "üîÑ Installing cached wheel: $(basename "$cached_wheel")"
                if pip install --no-cache-dir --disable-pip-version-check --quiet "$cached_wheel" 2>&1; then
                    if python -c "import nunchaku; print(f'‚úÖ Nunchaku {nunchaku.__version__} installed successfully from cached wheel')" 2>/dev/null; then
                        log "‚úÖ Nunchaku installation from cached wheel verified successfully"
                        return 0
                    else
                        log_error "‚ùå Cached wheel verification failed, removing and trying fresh download"
                        rm -f "$cached_wheel"
                    fi
                else
                    log_error "‚ùå Failed to install from cached wheel, removing and trying fresh download"
                    rm -f "$cached_wheel"
                fi
            fi
        fi
        
        # Download wheel from HuggingFace (the correct source)
        local nunchaku_wheel_url="https://huggingface.co/mit-han-lab/nunchaku/resolve/main/$nunchaku_wheel_name"
        
        log "Downloading Nunchaku wheel from: $nunchaku_wheel_url"
        log "Caching to: $cached_wheel"
        
        # Try multiple download methods
        local download_success=false
        local temp_wheel="/tmp/nunchaku_temp.whl"
        
        # Method 1: wget
        if wget -q --show-progress -O "$temp_wheel" "$nunchaku_wheel_url" 2>/dev/null; then
            download_success=true
            log "‚úÖ Nunchaku wheel downloaded with wget"
        fi
        
        # Method 2: curl (fallback)
        if [[ "$download_success" != "true" ]]; then
            log "üîÑ wget failed, trying curl..."
            if curl -L -s -o "$temp_wheel" "$nunchaku_wheel_url" 2>/dev/null; then
                download_success=true
                log "‚úÖ Nunchaku wheel downloaded with curl"
            fi
        fi
        
        # Method 3: pip direct install (last resort)
        if [[ "$download_success" != "true" ]]; then
            log "üîÑ Download failed, trying pip direct install..."
            if pip install --no-cache-dir --disable-pip-version-check --quiet "nunchaku" 2>/dev/null; then
                download_success=true
                log "‚úÖ Nunchaku installed directly via pip"
                return 0
            fi
        fi
        
        if [[ "$download_success" == "true" ]]; then
            log "‚úÖ Nunchaku wheel downloaded to temporary location"
            
            # Verify wheel file integrity BEFORE caching
            log "üîç Verifying wheel file integrity before caching..."
            local file_size=$(stat -f%z "$temp_wheel" 2>/dev/null || stat -c%s "$temp_wheel" 2>/dev/null || echo "0")
            if [[ "$file_size" -lt 1000000 ]]; then
                log "‚ö†Ô∏è Downloaded wheel too small ($file_size bytes) - likely download failed"
                rm -f "$temp_wheel"
                log "üîÑ Trying pip direct install with specific version..."
                if pip install --no-cache-dir --disable-pip-version-check --quiet "nunchaku==$nunchaku_version" 2>/dev/null; then
                    log "‚úÖ Nunchaku $nunchaku_version installed directly via pip (bypassing failed download)"
                    return 0
                else
                    log_error "‚ùå Direct pip install also failed, trying fallback installation..."
                    # Final fallback - try installing from git if version is not available
                    if pip install --no-cache-dir --disable-pip-version-check --quiet "git+https://github.com/mit-han-lab/nunchaku.git@v$nunchaku_version" 2>/dev/null; then
                        log "‚úÖ Nunchaku installed from git repository"
                        return 0
                    else
                        log_error "‚ùå All nunchaku installation methods failed"
                        return 1
                    fi
                fi
            elif ! python -c "import zipfile; zipfile.ZipFile('$temp_wheel').testzip()" 2>/dev/null; then
                log "‚ö†Ô∏è Downloaded wheel verification failed - wheel is corrupted"
                rm -f "$temp_wheel"
                log "üîÑ Trying pip direct install with specific version..."
                if pip install --no-cache-dir --disable-pip-version-check --quiet "nunchaku==$nunchaku_version" 2>/dev/null; then
                    log "‚úÖ Nunchaku $nunchaku_version installed directly via pip (bypassing corrupted wheel)"
                    return 0
                else
                    log_error "‚ùå Direct pip install also failed, trying fallback installation..."
                    # Final fallback - try installing from git if version is not available
                    if pip install --no-cache-dir --disable-pip-version-check --quiet "git+https://github.com/mit-han-lab/nunchaku.git@v$nunchaku_version" 2>/dev/null; then
                        log "‚úÖ Nunchaku installed from git repository"
                        return 0
                    else
                        log_error "‚ùå All nunchaku installation methods failed"
                        return 1
                    fi
                fi
            fi
            
            # Wheel is valid - now cache it and install
            log "‚úÖ Wheel verification successful - caching valid wheel"
            cp "$temp_wheel" "$cached_wheel"
            rm -f "$temp_wheel"  # Clean up temp file
            
            log "üîÑ Installing verified wheel: $(basename "$cached_wheel")"
            if pip install --no-cache-dir --disable-pip-version-check --quiet "$cached_wheel" 2>&1; then
                # Enhanced verification with multiple checks
                local verification_success=false
                
                # Check 1: Basic import
                if python -c "import nunchaku" 2>/dev/null; then
                    verification_success=true
                    log "‚úÖ Nunchaku basic import successful"
                fi
                
                # Check 2: Version check
                if [[ "$verification_success" == "true" ]]; then
                    if python -c "import nunchaku; print(f'‚úÖ Nunchaku {nunchaku.__version__} installed successfully')" 2>/dev/null; then
                        log "‚úÖ Nunchaku version verification successful"
                    else
                        log "‚ö†Ô∏è Nunchaku imported but version check failed"
                    fi
                fi
                
                # Check 3: Verify in virtual environment
                if [[ "$verification_success" == "true" ]]; then
                    local venv_python="${VENV_DIR:-/tmp}/sd_comfy-env/bin/python"
                    if [[ -f "$venv_python" ]]; then
                        if "$venv_python" -c "import nunchaku" 2>/dev/null; then
                            log "‚úÖ Nunchaku verified in virtual environment"
                        else
                            log "‚ö†Ô∏è Nunchaku not accessible in virtual environment"
                        fi
                    fi
                fi
                
                if [[ "$verification_success" == "true" ]]; then
                    log "‚úÖ Nunchaku installation verified successfully"
                    return 0
                else
                    log_error "‚ùå Nunchaku installation verification failed"
                    log_error "‚ö†Ô∏è ABI compatibility issue detected"
                    return 1
                fi
            else
                log_error "‚ùå Nunchaku wheel installation failed"
                return 1
            fi
        else
            log_error "‚ùå All download methods failed for Nunchaku wheel"
            log_error "This could be due to network issues or no wheel available for PyTorch ${torch_version_major_minor}"
            log "‚ö†Ô∏è Continuing without Nunchaku - some custom nodes may not work"
            return 1
        fi
    }
            
    install_hunyuan3d_texture_components() {
        local hunyuan3d_path="$REPO_DIR/custom_nodes/ComfyUI-Hunyuan3d-2-1"
    [[ ! -d "$hunyuan3d_path" ]] && return 1
    install_package "pybind11"
    install_package "ninja"
    for component in custom_rasterizer DifferentiableRenderer; do
        local comp_path="$hunyuan3d_path/hy3dpaint/$component"
        if [[ -d "$comp_path" ]]; then
            cd "$comp_path" || { log_error "$component directory access failed"; continue; }
            log "üîß Building $component (suppressing verbose output)..."
            python setup.py install >/tmp/${component}_build.log 2>&1 || log_error "$component installation failed - check /tmp/${component}_build.log"
            cd - > /dev/null
        fi
    done
}

    process_requirements() {
        local req_file="$1"
        [[ ! -f "$req_file" ]] && return 0
        log "üìã Processing requirements with enhanced caching: $req_file"
        
        # Use enhanced caching first (optional)
        if [[ "${ENABLE_ENHANCED_CACHE:-true}" == "true" ]]; then
            enhanced_requirements_cache "$req_file" || log "‚ö†Ô∏è Enhanced caching failed for $(basename "$req_file") but continuing"
        fi
        
        # Try batch installation first with wheel cache
        if pip install --quiet --find-links "/storage/.wheel_cache" -r "$req_file" 2>/dev/null; then
            log "‚úÖ Batch installation successful (using cached wheels)"
            return 0
        fi
        
        # If batch fails, try with different strategies instead of individual packages
        log "‚ö†Ô∏è Batch install failed, trying optimized batch strategies..."
        
        # Strategy 1: Try with --no-deps to bypass dependency resolver
        log "üîÑ Strategy 1: Installing with --no-deps..."
        if pip install --no-deps --quiet -r "$req_file" 2>/dev/null; then
            log "‚úÖ Strategy 1 successful (--no-deps)"
            return 0
        fi
        
        # Strategy 2: Try with --force-reinstall
        log "üîÑ Strategy 2: Installing with --force-reinstall..."
        if pip install --force-reinstall --quiet -r "$req_file" 2>/dev/null; then
            log "‚úÖ Strategy 2 successful (--force-reinstall)"
            return 0
        fi
        
        # Strategy 3: Try with --ignore-installed
        log "üîÑ Strategy 3: Installing with --ignore-installed..."
        if pip install --ignore-installed --quiet -r "$req_file" 2>/dev/null; then
            log "‚úÖ Strategy 3 successful (--ignore-installed)"
            return 0
        fi
        
        # Strategy 4: Try with --no-cache-dir
        log "üîÑ Strategy 4: Installing with --no-cache-dir..."
        if pip install --no-cache-dir --quiet -r "$req_file" 2>/dev/null; then
            log "‚úÖ Strategy 4 successful (--no-cache-dir)"
            return 0
        fi
        
        # Strategy 5: Try with --pre (include pre-releases)
        log "üîÑ Strategy 5: Installing with --pre (including pre-releases)..."
        if pip install --pre --quiet -r "$req_file" 2>/dev/null; then
            log "‚úÖ Strategy 5 successful (--pre)"
            return 0
        fi
        
        # Final fallback: individual packages (only if all batch strategies fail)
        log "‚ö†Ô∏è All batch strategies failed, falling back to individual packages..."
        while read -r pkg; do 
            [[ "$pkg" =~ ^[[:space:]]*# ]] || [[ -z "$pkg" ]] || install_package "$pkg" || true
        done < "$req_file"
    }

# Service loop function (from original)
service_loop() { while true; do eval "$1"; sleep 1; done; }

# Prepare symlinks (simplified from original)
# Fix directory and filesystem issues
fix_directory_issues() {
    log "üîß Fixing directory and filesystem issues..."
    
    # Ensure model directories exist and have correct permissions
    local model_dirs=(
        "$REPO_DIR/models/LLM"
        "$REPO_DIR/models/ultralytics/bbox"
        "$REPO_DIR/models/ultralytics/segm"
        "$REPO_DIR/models/checkpoints"
        "$REPO_DIR/models/loras"
        "$REPO_DIR/models/vae"
        "$REPO_DIR/models/controlnet"
        "$REPO_DIR/models/embeddings"
    )
    
    for dir in "${model_dirs[@]}"; do
        if [[ -f "$dir" ]]; then
            log "‚ö†Ô∏è Removing file blocking directory creation: $dir"
            rm -f "$dir"
        fi
        mkdir -p "$dir" 2>/dev/null || true
        chmod 755 "$dir" 2>/dev/null || true
    done
    
    # Fix common permission issues
    chmod -R 755 "$REPO_DIR/custom_nodes" 2>/dev/null || true
    
    log "‚úÖ Directory issues fixed"
}

prepare_link() {
    for link_pair in "$@"; do
        [[ "$link_pair" =~ ^(.+):(.+)$ ]] && {
            local source="${BASH_REMATCH[1]}" target="${BASH_REMATCH[2]}"
            [[ -n "$source" && -n "$target" ]] && {
                mkdir -p "$(dirname "$target")"
                [[ ! -L "$target" ]] && ln -sf "$source" "$target" 2>/dev/null || true
            }
        }
    done
}

# Cache Management - Keep cache under specified size limit
manage_cache_size() {
    local cache_dirs=("/storage/.pip_cache" "/storage/.wheel_cache" "/storage/.apt_cache" "/storage/.sageattention_cache")
    local max_size_gb=${1:-7}  # Default 7GB limit
    local max_size_bytes=$((max_size_gb * 1024 * 1024 * 1024))
    
    log "üßπ Managing cache sizes (limit: ${max_size_gb}GB)..."
    
    for cache_dir in "${cache_dirs[@]}"; do
        [[ ! -d "$cache_dir" ]] && continue
        
        local current_size=$(du -sb "$cache_dir" 2>/dev/null | cut -f1 || echo "0")
        local current_size_mb=$((current_size / 1024 / 1024))
        
        if [[ $current_size -gt $((max_size_bytes / 4)) ]]; then
            log "üìä Cache $cache_dir: ${current_size_mb}MB (cleaning old files...)"
            
            # Remove files older than 7 days
            find "$cache_dir" -type f -mtime +7 -delete 2>/dev/null || true
            
            # If still too large, remove oldest files
            while [[ $(du -sb "$cache_dir" 2>/dev/null | cut -f1 || echo "0") -gt $((max_size_bytes / 4)) ]]; do
                local oldest_file=$(find "$cache_dir" -type f -printf '%T@ %p\n' 2>/dev/null | sort -n | head -1 | cut -f2- -d' ')
                [[ -n "$oldest_file" ]] && rm -f "$oldest_file" 2>/dev/null || break
            done
            
            local new_size=$(du -sb "$cache_dir" 2>/dev/null | cut -f1 || echo "0")
            local new_size_mb=$((new_size / 1024 / 1024))
            log "‚úÖ Cache $cache_dir cleaned: ${new_size_mb}MB"
        else
            log "‚úÖ Cache $cache_dir: ${current_size_mb}MB (within limits)"
        fi
    done
}

# Enhanced Requirements Caching - Pre-download and cache all requirements
enhanced_requirements_cache() {
    local req_file="$1"
    local cache_dir="/storage/.requirements_cache"
    local wheel_cache="/storage/.wheel_cache"
    
    [[ ! -f "$req_file" ]] && return 0
    
    log "üöÄ Enhanced requirements caching for: $(basename "$req_file")"
    mkdir -p "$cache_dir" "$wheel_cache"
    
    # Create cache key based on requirements file hash
    local req_hash=$(md5sum "$req_file" 2>/dev/null | cut -d' ' -f1 || echo "unknown")
    local cache_marker="$cache_dir/cached_${req_hash}.marker"
    
    # Check if this exact requirements set is already cached
    if [[ -f "$cache_marker" ]]; then
        log "‚úÖ Requirements already cached (hash: $req_hash), using cached packages..."
        return 0
    fi
    
    # Pre-download wheels in smaller batches to avoid failures
    log "üì• Pre-downloading and caching wheels in batches..."
    local download_temp="/tmp/req_download_$$"
    mkdir -p "$download_temp"
    
    # Debug info if enabled
    if [[ "${DEBUG_CACHE:-false}" == "true" ]]; then
        log "üîç Debug: Requirements file has $(wc -l < "$req_file") lines"
        log "üîç Debug: Download temp dir: $download_temp"
        log "üîç Debug: Wheel cache dir: $wheel_cache"
        log "üîç Debug: Available disk space: $(df -h /tmp | tail -1 | awk '{print $4}')"
    fi
    
    # Create clean requirements file (remove git+, comments, empty lines)
    local clean_req="/tmp/clean_req_$$.txt"
    grep -v '^[[:space:]]*#' "$req_file" | \
    grep -v '^[[:space:]]*$' | \
    grep -v '^[[:space:]]*-e' | \
    grep -v 'git+http' | \
    grep -v '^[[:space:]]*--' > "$clean_req"
    
    local total_wheels=0
    local failed_downloads=0
    
    # Split into smaller batches of 10 packages each
    split -l 10 "$clean_req" "/tmp/req_batch_"
    
    for batch_file in /tmp/req_batch_*; do
        [[ ! -f "$batch_file" ]] && continue
        
        log "üì¶ Pre-downloading batch: $(basename "$batch_file")"
        
        # Try downloading this batch
        local batch_log="/tmp/pip_download_$(basename "$batch_file").log"
        if timeout 60s pip download --dest "$download_temp" --no-deps -r "$batch_file" 2>"$batch_log"; then
            log "‚úÖ Batch download successful: $(basename "$batch_file")"
        else
            if [[ "${DEBUG_CACHE:-false}" == "true" ]]; then
                log "üîç Debug: Batch download error for $(basename "$batch_file"):"
                [[ -f "$batch_log" ]] && tail -3 "$batch_log" | while read line; do log "    $line"; done
            fi
            log "‚ö†Ô∏è Batch download failed: $(basename "$batch_file") (trying individual packages)"
            ((failed_downloads++))
            
            # Try individual packages in failed batch
            while read -r pkg; do
                [[ -z "$pkg" ]] && continue
                local pkg_name=$(echo "$pkg" | sed 's/[<>=!].*//' | sed 's/\[.*\]//')
                
                if timeout 30s pip download --dest "$download_temp" --quiet --no-deps "$pkg" 2>/dev/null; then
                    log "  ‚úÖ Individual download: $pkg_name"
                else
                    log "  ‚ö†Ô∏è Failed individual download: $pkg_name"
                fi
            done < "$batch_file"
        fi
    done
    
    # Move downloaded wheels to cache
    local wheels_cached=0
    if [[ -d "$download_temp" ]]; then
        for wheel in "$download_temp"/*.whl; do
            [[ -f "$wheel" ]] && cp "$wheel" "$wheel_cache/" && ((wheels_cached++))
        done
        
        # Also try to find any tar.gz files and note them
        local source_packages=$(find "$download_temp" -name "*.tar.gz" | wc -l)
        
        if [[ $wheels_cached -gt 0 ]]; then
            log "üíæ Cached $wheels_cached wheels for future use"
            
            # Create cache marker only if we got some wheels
            echo "Cached requirements on $(date)" > "$cache_marker"
            echo "Requirements hash: $req_hash" >> "$cache_marker"
            echo "Wheels cached: $wheels_cached" >> "$cache_marker"
            echo "Source packages found: $source_packages" >> "$cache_marker"
            echo "Failed batches: $failed_downloads" >> "$cache_marker"
        else
            log "‚ö†Ô∏è No wheels downloaded - packages may need compilation or be unavailable"
        fi
    else
        log "‚ö†Ô∏è Download directory not created - pre-download completely failed"
    fi
    
    # Clean up temp files
    rm -rf "$download_temp" "$clean_req" /tmp/req_batch_* /tmp/pip_download_*.log
}

# Send to Discord (simplified)
send_to_discord() {
    [[ -n "$DISCORD_WEBHOOK_URL" ]] && curl -X POST -H "Content-Type: application/json" -d "{\"content\":\"$1\"}" "$DISCORD_WEBHOOK_URL" &>/dev/null || true
}

# Fix PyTorch version alignment between installation and runtime
fix_pytorch_version_alignment() {
    log "üîß Checking PyTorch version alignment..."
    
    # Check if there's a version mismatch
    local installed_version=$(python -c "import torch; print(torch.__version__)" 2>/dev/null || echo "none")
    local target_version="$TORCH_VERSION"
    
    if [[ "$installed_version" != "none" && "$installed_version" != "$target_version" ]]; then
        log "‚ö†Ô∏è PyTorch version mismatch detected:"
        log "   Installed: $installed_version"
        log "   Target:    $target_version"
        log "üîÑ Aligning PyTorch versions..."
        
        # Remove existing PyTorch installation
        pip uninstall -y torch torchvision torchaudio xformers 2>/dev/null || true
        
        # Clear PyTorch cache
        rm -rf /storage/.torch_extensions 2>/dev/null || true
        rm -rf ~/.cache/torch 2>/dev/null || true
        
        # Force reinstall with correct version
        pip install --no-cache-dir --disable-pip-version-check --force-reinstall --quiet \
            torch==$target_version torchvision==$TORCHVISION_VERSION torchaudio==$TORCHAUDIO_VERSION \
            --extra-index-url "https://download.pytorch.org/whl/cu126" 2>/dev/null || log_error "PyTorch reinstall failed"
        
        # Verify alignment
        local new_version=$(python -c "import torch; print(torch.__version__)" 2>/dev/null || echo "none")
        if [[ "$new_version" == "$target_version" ]]; then
            log "‚úÖ PyTorch version aligned successfully: $new_version"
        else
            log_error "‚ùå PyTorch version alignment failed: $new_version"
        fi
    else
        log "‚úÖ PyTorch version already aligned: $installed_version"
    fi
}

# Create combined requirements file from all custom nodes
create_combined_requirements_file() {
    local combined_req_file="/tmp/all_custom_node_requirements.txt"
    local nodes_dir="$REPO_DIR/custom_nodes"
    
    # Clear previous file
    > "$combined_req_file"
    
    log "üîç Scanning custom nodes for requirements.txt files..."
    local req_files_found=0
    
    # Find all requirements.txt files in custom nodes
    if [[ -d "$nodes_dir" ]]; then
        while IFS= read -r -d '' req_file; do
            local node_name=$(basename "$(dirname "$req_file")")
            log "üì¶ Found requirements.txt in $node_name"
            
            # Process each line to clean up common issues
            while IFS= read -r line; do
                # Skip empty lines and comments
                [[ -z "$line" ]] && continue
                [[ "$line" =~ ^[[:space:]]*# ]] && continue
                
                # Clean up common malformed entries
                local cleaned_line="$line"
                
                # Skip editable installs
                [[ "$line" =~ ^[[:space:]]*-e ]] && continue
                
                # Fix common malformed names
                if [[ "$line" == *"accelerateopencv-python"* ]]; then
                    cleaned_line="accelerate opencv-python"
                    log "üîß Fixed malformed package in $node_name: $line -> $cleaned_line"
                fi
                
                # Remove trailing comments and whitespace
                cleaned_line=$(echo "$cleaned_line" | sed 's/#.*$//' | sed 's/^[[:space:]]*//;s/[[:space:]]*$//')
                
                # Skip if empty after cleaning
                [[ -z "$cleaned_line" ]] && continue
                
                # Add cleaned line to combined file
                echo "$cleaned_line" >> "$combined_req_file"
            done < "$req_file"
            
            ((req_files_found++))
        done < <(find "$nodes_dir" -name "requirements.txt" -print0 2>/dev/null)
    fi
    
    log "üìã Combined requirements file created: $combined_req_file ($req_files_found nodes)"
    
    # Remove duplicates and clean up
    if [[ -s "$combined_req_file" ]]; then
        sort -u "$combined_req_file" -o "$combined_req_file"
        log "üßπ Cleaned up duplicate requirements"
        
        # Show summary of cleaned requirements
        local total_packages=$(wc -l < "$combined_req_file")
        log "üìä Total unique packages to process: $total_packages"
    fi
}

# Fast batch installer for critical dependencies with enhanced caching
install_critical_dependencies() {
    log "üì¶ Installing Critical Custom Node Dependencies with Enhanced Caching..."
    local critical_packages=("blend_modes" "deepdiff" "rembg" "webcolors" "ultralytics" "inflect" "soxr" "groundingdino" "insightface" "opencv-python" "opencv-contrib-python" "facexlib" "onnxruntime" "timm" "segment-anything" "scikit-image" "piexif" "transformers" "opencv-python-headless" "scipy>=1.11.4" "numpy" "dill" "matplotlib" "oss2" "gguf" "diffusers" "huggingface_hub>=0.34.0" "pytorch_lightning" "sounddevice" "av>=12.0.0,<14.0.0" "accelerate")
    
    # Create temporary requirements file for enhanced caching
    local temp_critical_req="/tmp/critical_deps_$(date +%s).txt"
    printf "%s\n" "${critical_packages[@]}" > "$temp_critical_req"
    
    # Use enhanced caching for critical dependencies (optional)
    if [[ "${ENABLE_ENHANCED_CACHE:-true}" == "true" ]]; then
        enhanced_requirements_cache "$temp_critical_req" || log "‚ö†Ô∏è Enhanced caching failed for critical dependencies but continuing"
    fi
    
    # Fast check for already installed packages using pip list
    log "üîç Fast-checking already installed packages..."
    local installed_packages=$(pip list --quiet | grep -E "^($(echo "${critical_packages[@]}" | tr ' ' '|' | sed 's/[<>=!].*//g')) " | cut -d' ' -f1)
    local already_installed=($installed_packages)
    
    # Filter out already installed packages
    local packages_to_install=()
    for pkg in "${critical_packages[@]}"; do
        local pkg_name=$(echo "$pkg" | sed 's/[<>=!].*//' | sed 's/\[.*\]//')
        if [[ ! " ${already_installed[@]} " =~ " ${pkg_name} " ]]; then
            packages_to_install+=("$pkg")
        else
            log "‚úÖ $pkg_name already installed (skipping)"
        fi
    done
    
    # Batch install remaining packages
    if [[ ${#packages_to_install[@]} -gt 0 ]]; then
        log "üì¶ Installing ${#packages_to_install[@]} packages in batch..."
        
        # Create temporary requirements file for batch installation
        local temp_req_file="/tmp/critical_deps_$(date +%s).txt"
        printf "%s\n" "${packages_to_install[@]}" > "$temp_req_file"
        
        # Batch install with optimized flags and wheel cache
        if pip install --no-cache-dir --disable-pip-version-check --quiet --progress-bar off --find-links "/storage/.wheel_cache" -r "$temp_req_file" 2>/dev/null; then
            log "‚úÖ Batch installation successful for ${#packages_to_install[@]} packages (using cached wheels)"
        else
            log "‚ö†Ô∏è Batch installation failed, falling back to individual installation..."
            # Fallback to individual installation
            local installed_count=0
            local failed_count=0
            
            for pkg in "${packages_to_install[@]}"; do
                if install_package "$pkg"; then
                    ((installed_count++))
                else
                    ((failed_count++))
                fi
            done
            
            log "üìä Fallback installation summary: $installed_count installed, $failed_count failed"
        fi
        
        # Clean up
        rm -f "$temp_req_file" "$temp_critical_req"
    else
        log "‚úÖ All critical packages already installed"
        rm -f "$temp_critical_req"
    fi
    
    # Final verification of critical dependencies
    log "üîç Final verification of critical dependencies..."
    verify_critical_dependencies
}

# Fast verification of critical dependencies
verify_critical_dependencies() {
    log "üîç Verifying critical dependencies..."
    local verification_failures=0
    
    # Critical packages to verify
    local critical_packages=("torch" "diffusers" "gguf" "nunchaku" "huggingface_hub")
    
    # Fast batch verification using pip list
    log "üîç Fast-verifying package installations..."
    local installed_packages=$(pip list --quiet | grep -E "^($(echo "${critical_packages[@]}" | tr ' ' '|')) " | cut -d' ' -f1)
    
    for pkg in "${critical_packages[@]}"; do
        local pkg_name=$(echo "$pkg" | sed 's/[<>=!].*//' | sed 's/\[.*\]//')
        
        if echo "$installed_packages" | grep -q "^$pkg_name$"; then
            log "‚úÖ $pkg_name: Installation verified"
        else
            log_error "‚ùå $pkg_name: Installation failed"
            ((verification_failures++))
        fi
    done
    
    # Special verification for PyTorch CUDA (only if torch is installed)
    if echo "$installed_packages" | grep -q "^torch$"; then
        if python -c "import torch; print(f'PyTorch {torch.__version__} CUDA: {torch.cuda.is_available()}')" 2>/dev/null; then
            log "‚úÖ PyTorch CUDA verification successful"
        else
            log_error "‚ùå PyTorch CUDA verification failed"
            ((verification_failures++))
        fi
    else
        log_error "‚ùå PyTorch not installed for CUDA verification"
        ((verification_failures++))
    fi
    
    if [[ $verification_failures -gt 0 ]]; then
        log_error "‚ö†Ô∏è $verification_failures critical dependency verification(s) failed"
        log_error "Custom nodes may not work properly"
    else
        log "‚úÖ All critical dependencies verified successfully"
    fi
    
    return $verification_failures
}

#######################################
# MAIN EXECUTION FLOW
#######################################

main() {
    test_connectivity || {
        log_error "Network connectivity test failed. Check your internet connection."
        exit 1
    }
    
    if [[ -f "/tmp/sd_comfy.prepared" && -z "$REINSTALL_SD_COMFY" ]]; then
        activate_global_venv
                    return 0
    fi
    
    setup_cuda_env
    install_cuda
    
    cd "$REPO_DIR"
    export TARGET_REPO_URL="https://github.com/comfyanonymous/ComfyUI.git"
    export TARGET_REPO_DIR=$REPO_DIR
    export UPDATE_REPO=$SD_COMFY_UPDATE_REPO
    export UPDATE_REPO_COMMIT=$SD_COMFY_UPDATE_REPO_COMMIT
    
    [[ -d ".git" ]] && { [[ -n "$(git status --porcelain requirements.txt 2>/dev/null)" ]] && git checkout -- requirements.txt; git symbolic-ref -q HEAD >/dev/null || git checkout main || git checkout master || git checkout -b main; git fetch --all &>/dev/null && git reset --hard origin/HEAD &>/dev/null; }
    
    # Fix directory and filesystem issues first
    fix_directory_issues
    
    prepare_link "$REPO_DIR/output:$IMAGE_OUTPUTS_DIR/stable-diffusion-comfy" "$MODEL_DIR:$WORKING_DIR/models" "$MODEL_DIR/sd:$LINK_MODEL_TO" "$MODEL_DIR/lora:$LINK_LORA_TO" "$MODEL_DIR/vae:$LINK_VAE_TO" "$MODEL_DIR/upscaler:$LINK_UPSCALER_TO" "$MODEL_DIR/controlnet:$LINK_CONTROLNET_TO" "$MODEL_DIR/embedding:$LINK_EMBEDDING_TO" "$MODEL_DIR/llm_checkpoints:$LINK_LLM_TO"
    
    # Activate global virtual environment (will create if needed)
    activate_global_venv
    
    # Optimize pip for speed
    log "‚ö° Optimizing pip for maximum speed..."
    pip config set global.progress_bar off 2>/dev/null || true
    pip config set global.quiet true 2>/dev/null || true
    pip config set global.disable_pip_version_check true 2>/dev/null || true
    pip config set global.no_cache_dir true 2>/dev/null || true
    
    # Manage cache sizes to stay under 7GB limit
    manage_cache_size 7
    
    # Enhanced caching can be disabled by setting ENABLE_ENHANCED_CACHE=false
    # Debug caching issues by setting DEBUG_CACHE=true
    if [[ "${ENABLE_ENHANCED_CACHE:-true}" == "false" ]]; then
        log "‚ö†Ô∏è Enhanced caching disabled (ENABLE_ENHANCED_CACHE=false)"
    fi
    
    # Enhanced system dependencies installation with caching
    install_system_dependencies
    
    # Fix PyTorch version alignment issues
    fix_pytorch_version_alignment
    
    setup_pytorch
    for component in "sageattention" "nunchaku" "hunyuan3d_texture_components"; do install_component "$component"; done
    
    # Update ComfyUI components FIRST - before any dependency installation
    log "üöÄ Updating ComfyUI components FIRST (before dependencies)..."
    update_comfyui_manager || log_error "ComfyUI Manager update had issues (continuing)"
    update_custom_nodes || log_error "Custom nodes update had issues (continuing)"
    
    # NOW install dependencies AFTER nodes are updated
    log "üì¶ Installing dependencies AFTER node updates..."
    resolve_dependencies || log_error "Some dependencies failed (continuing)"
    
    # Process core ComfyUI requirements
    process_requirements "$REPO_DIR/requirements.txt" || log_error "Core requirements had issues (continuing)"
    
    # Process combined custom node requirements (this was missing!)
    log "üîß Processing combined custom node requirements..."
    
    # Create combined requirements file from all custom nodes
    log "üìã Collecting requirements from all custom nodes..."
    create_combined_requirements_file
    
    # Process the combined requirements with enhanced caching
    log "üöÄ Using enhanced caching for faster requirements processing..."
    process_combined_requirements "/tmp/all_custom_node_requirements.txt" || log_error "Combined requirements processing had issues (continuing)"
    
    # Handle specific dependency conflicts AFTER all requirements are processed
    log "üîß Handling specific dependency conflicts..."
    
    # Fix xformers conflicts
    log "üîÑ Fixing xformers conflicts..."
    pip uninstall -y xformers 2>/dev/null || true
    pip install --quiet -U xformers --index-url https://download.pytorch.org/whl/cu128 --index-url https://pypi.org/simple 2>/dev/null || log_error "xformers fix failed"
    
    # Fix other common conflicts
    log "üîß Fixing other dependency conflicts..."
    pip install --quiet --upgrade torchaudio torchvision 2>/dev/null || log_error "torch audio/vision fix failed"
    pip install --quiet --upgrade timm==1.0.13 2>/dev/null || log_error "timm fix failed"
    
    # Reinstall flet to fix version issues
    log "üîÑ Reinstalling flet to fix version conflicts..."
    pip uninstall -y flet 2>/dev/null || true
    pip install --quiet flet==0.23.2 2>/dev/null || log_error "flet fix failed"
    
    # Fix critical huggingface_hub compatibility issues
    log "üîß Fixing huggingface_hub compatibility issues..."
    pip uninstall -y huggingface_hub diffusers 2>/dev/null || true
    pip install --quiet "huggingface_hub>=0.34.0" 2>/dev/null || log_error "huggingface_hub fix failed"
    pip install --quiet "diffusers" 2>/dev/null || log_error "diffusers fix failed"
    
    # Fix PyAV version issues for API nodes
    log "üîß Fixing PyAV version for API nodes..."
    pip uninstall -y av 2>/dev/null || true
    pip install --quiet "av>=12.0.0,<14.0.0" 2>/dev/null || log_error "PyAV fix failed"
    
    # Ensure correct nunchaku version is installed
    log "üîß Ensuring correct nunchaku version..."
    local current_nunchaku=$(pip show nunchaku 2>/dev/null | grep Version | cut -d' ' -f2)
    if [[ "$current_nunchaku" != "0.3.1" ]]; then
        log "‚ö†Ô∏è Wrong nunchaku version detected: $current_nunchaku, fixing..."
        pip uninstall -y nunchaku 2>/dev/null || true
        pip install --quiet "nunchaku==0.3.1" 2>/dev/null || log_error "nunchaku version fix failed"
    fi
    
    # CRITICAL: Install missing dependencies AFTER batch requirements processing
    log "üöÄ Installing Critical Custom Node Dependencies (AFTER batch requirements)..."
    install_critical_dependencies
    
    # Final cache cleanup and optimization
    log "üßπ Final cache optimization..."
    manage_cache_size 7
    
    touch "/tmp/sd_comfy.prepared"
}

# Execute main workflow
main

# Model download (can run in background)
download_models() {
    [[ -n "$SKIP_MODEL_DOWNLOAD" ]] && { log "‚è≠Ô∏è Model download skipped (SKIP_MODEL_DOWNLOAD set)"; return; }
    log "üì• Starting model download process... üí° Models will download in background while ComfyUI is running üí° You can start using ComfyUI immediately!"
    bash "$SCRIPT_DIR/../utils/sd_model_download/main.sh" & local download_pid=$!
    log "üìã Model download started with PID: $download_pid üìã Check progress with: tail -f $LOG_DIR/sd_comfy.log"
        echo "$download_pid" > /tmp/model_download.pid
}

# Launch ComfyUI
launch() {
    [[ -n "$INSTALL_ONLY" ]] && return 0
    log "Launching ComfyUI..."
  cd "$REPO_DIR"
  
    # Log rotation
    [[ -f "$LOG_DIR/sd_comfy.log" ]] && { local timestamp=$(date +"%Y%m%d_%H%M%S"); mv "$LOG_DIR/sd_comfy.log" "$LOG_DIR/sd_comfy_${timestamp}.log"; ls -t "$LOG_DIR"/sd_comfy_*.log 2>/dev/null | tail -n +6 | xargs -r rm; }
  
    # A4000-specific optimizations
  export PYTORCH_CUDA_ALLOC_CONF="max_split_size_mb:4096,garbage_collection_threshold:0.8"
  
    # Runtime PyTorch verification (skip if fresh install)
    [[ ! -f "/tmp/pytorch_ecosystem_fresh_install" ]] && setup_pytorch || rm -f "/tmp/pytorch_ecosystem_fresh_install"
    
    # Check if SageAttention is available and adjust launch parameters accordingly
    local sage_attention_arg=""
    if python -c "import sageattention" 2>/dev/null; then
        sage_attention_arg="--use-sage-attention"
        log "‚úÖ SageAttention detected - enabling in ComfyUI"
    else
        log "‚ö†Ô∏è SageAttention not available - launching without it"
    fi
    
    # Launch ComfyUI with optimized parameters (using globally activated venv Python)
    PYTHONUNBUFFERED=1 service_loop "python main.py --dont-print-server --port $SD_COMFY_PORT --cuda-malloc $sage_attention_arg --preview-method auto --bf16-vae --fp16-unet --cache-lru 5 --reserve-vram 0.5 --fast --enable-compress-response-body ${EXTRA_SD_COMFY_ARGS}" > "$LOG_DIR/sd_comfy.log" 2>&1 &
  echo $! > /tmp/sd_comfy.pid
}

# Error logging (simplified)
log_errors() {
    log "üìù Installation completed - check logs above for any errors"
}

# Log any errors before launch
log_errors

# Start ComfyUI first (so user can access it immediately)
log "üöÄ Starting ComfyUI first for immediate access..."
launch

# Download models in background (non-blocking)
log "üì• Starting model download in background..."
download_models &

# Wait a moment for ComfyUI to start
sleep 5

# Final notifications (from original)
send_to_discord "Stable Diffusion Comfy Started"
env | grep -q "PAPERSPACE" && send_to_discord "Link: https://$PAPERSPACE_FQDN/sd-comfy/"

# Show final status
log ""
log "üéâ COMFYUI STARTUP COMPLETE!"
log "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
log "‚úÖ ComfyUI is now running and accessible"
log "üì• Model download is running in background"
log "üîó Access ComfyUI at: http://localhost:$SD_COMFY_PORT"

# Show Paperspace URL if available
if env | grep -q "PAPERSPACE"; then
    log "üåê Paperspace URL: https://$PAPERSPACE_FQDN/sd-comfy/"
fi

log ""
log "üìã Useful commands:"
log "  ‚Ä¢ Check ComfyUI logs: tail -f $LOG_DIR/sd_comfy.log"
log "  ‚Ä¢ Check model download: tail -f /tmp/model_download.log"
log "  ‚Ä¢ Stop model download: kill \$(cat /tmp/model_download.pid)"
log "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"

[[ -n "${CF_TOKEN}" ]] && { [[ "$RUN_SCRIPT" != *"sd_comfy"* ]] && export RUN_SCRIPT="$RUN_SCRIPT,sd_comfy"; bash "$SCRIPT_DIR/../cloudflare_reload.sh"; }
